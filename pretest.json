{
  "version": 2.0,
  "questions": [
    {
      "question": "What is the primary cause of overfitting in machine learning models?",
      "answers": {
        "a": "Insufficient training data",
        "b": "Using too few parameters in the model",
        "c": "Learning noise instead of general patterns",
        "d": "Using a very simple model"
      },
      "explanations": {
        "a": "More data helps generalization but isn't the primary cause.",
        "b": "Too few parameters usually cause underfitting, not overfitting.",
        "c": "Correct! Overfitting happens when the model memorizes the noise instead of learning general patterns.",
        "d": "A simple model usually leads to underfitting, not overfitting."
      },
      "correctAnswer": "c",
      "difficulty": "beginner"
    },
    {
      "question": "Which of the following is a common symptom of underfitting?",
      "answers": {
        "a": "High training accuracy but low test accuracy",
        "b": "Low accuracy on both training and test data",
        "c": "Model performing exceptionally well on unseen data",
        "d": "A high number of parameters leading to complex decision boundaries"
      },
      "explanations": {
        "a": "This is a sign of overfitting, not underfitting.",
        "b": "Correct! Underfitting happens when the model is too simple to capture patterns, leading to poor accuracy on both datasets.",
        "c": "A well-generalized model should perform well, but not exceptionally better than on training data.",
        "d": "A high number of parameters usually leads to overfitting."
      },
      "correctAnswer": "b",
      "difficulty": "beginner"
    },
    {
      "question": "Why is a train/test split necessary in machine learning?",
      "answers": {
        "a": "To make the model memorize the test data",
        "b": "To evaluate the model's performance on unseen data",
        "c": "To prevent the model from learning any patterns",
        "d": "To increase the training accuracy"
      },
      "explanations": {
        "a": "Memorizing test data leads to overfitting, which we want to avoid.",
        "b": "Correct! A train/test split helps assess the model's generalization ability.",
        "c": "The goal is to learn patterns, but avoid memorization.",
        "d": "A train/test split is for evaluation, not improving training accuracy."
      },
      "correctAnswer": "b",
      "difficulty": "beginner"
    },
    {
      "question": "What is the primary difference between supervised and unsupervised learning?",
      "answers": {
        "a": "Supervised learning requires labeled data, while unsupervised learning does not",
        "b": "Unsupervised learning uses neural networks, while supervised learning does not",
        "c": "Supervised learning is only used for regression tasks",
        "d": "Unsupervised learning always provides better accuracy"
      },
      "explanations": {
        "a": "Correct! Supervised learning works with labeled data, while unsupervised learning identifies patterns without labels.",
        "b": "Both learning types can use neural networks.",
        "c": "Supervised learning is used for both classification and regression.",
        "d": "Accuracy depends on the problem; one is not always better."
      },
      "correctAnswer": "a",
      "difficulty": "beginner"
    },
    {
      "question": "When a model performs very well on training data but poorly on new data, it is called:",
      "answers": {
        "a": "Underfitting",
        "b": "Overfitting",
        "c": "Good generalization",
        "d": "High bias"
      },
      "explanations": {
        "a": "Underfitting shows poor performance on training data too.",
        "b": "Correct! This large gap between training and test performance is the definition of overfitting.",
        "c": "Good generalization means similar performance on both.",
        "d": "High bias alone would hurt training performance as well."
      },
      "correctAnswer": "b",
      "difficulty": "beginner"
    },
    {
      "question": "Which of these is usually the best first step to fix overfitting?",
      "answers": {
        "a": "Make the model much more complex",
        "b": "Get more training data",
        "c": "Remove all regularization",
        "d": "Use a linear model only"
      },
      "explanations": {
        "a": "Increasing complexity usually makes overfitting worse.",
        "b": "Correct! More diverse training data is often the most effective and simplest way to reduce overfitting.",
        "c": "Removing regularization increases overfitting.",
        "d": "A linear model may cause underfitting instead."
      },
      "correctAnswer": "b",
      "difficulty": "beginner"
    }
  ]
}